{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9fd263f-0f92-490b-b8ce-5a67f3f6e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        scaler,\n",
    "        channel,\n",
    "        num_nodes,\n",
    "        seq_len,\n",
    "        pred_len,\n",
    "        dropout_n,\n",
    "        d_llm,\n",
    "        e_layer,\n",
    "        head,\n",
    "        lrate,\n",
    "        wdecay,\n",
    "        feature_w,\n",
    "        fcst_w,\n",
    "        recon_w,\n",
    "        att_w,\n",
    "        device,\n",
    "        epochs\n",
    "    ):\n",
    "        self.model = Dual(\n",
    "            device=device, channel=channel, num_nodes=num_nodes, seq_len=seq_len, pred_len=pred_len, \n",
    "            dropout_n=dropout_n, d_llm=d_llm, e_layer=e_layer, head=head\n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=min(epochs, 100), eta_min=1e-8, verbose=True)\n",
    "        self.MSE = MSE\n",
    "        self.MAE = MAE\n",
    "        self.clip = 5\n",
    "        self.scaler = scaler\n",
    "        self.device = device\n",
    "\n",
    "        self.feature_loss = 'smooth_l1'  \n",
    "        self.fcst_loss = 'smooth_l1'\n",
    "        self.recon_loss = 'smooth_l1'\n",
    "        self.att_loss = 'smooth_l1'   \n",
    "        self.fcst_w = 1\n",
    "        self.recon_w = 0.5\n",
    "        self.feature_w = 0.1     \n",
    "        self.att_w = 0.01\n",
    "        self.criterion = KDLoss(self.feature_loss, self.fcst_loss, self.recon_loss, self.att_loss,  self.feature_w,  self.fcst_w,  self.recon_w,  self.att_w)\n",
    "\n",
    "        print(\"The number of trainable parameters: {}\".format(self.model.count_trainable_params()))\n",
    "        print(\"The number of parameters: {}\".format(self.model.param_num()))\n",
    "        print(self.model)\n",
    "\n",
    "    def train(self, x, y, emb):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        ts_enc, prompt_enc, ts_out, prompt_out, ts_att, prompt_att = self.model(x, emb)\n",
    "        loss = self.criterion(ts_enc, prompt_enc, ts_out, prompt_out, ts_att, prompt_att, y)\n",
    "        loss.backward()\n",
    "        if self.clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip) \n",
    "        self.optimizer.step() \n",
    "        mse = self.MSE(ts_out, y) \n",
    "        mae = self.MAE(ts_out, y)\n",
    "        return loss.item(), mse.item(), mae.item()\n",
    "\n",
    "    def eval(self, x, y, emb):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            ts_enc, prompt_enc, ts_out, prompt_out, ts_att, prompt_att = self.model(x, emb)\n",
    "            loss = self.criterion(ts_enc, prompt_enc, ts_out, prompt_out, ts_att, prompt_att, y)\n",
    "            mse = self.MSE(ts_out, y)\n",
    "            mae = self.MAE(ts_out, y)\n",
    "        return loss.item(), mse.item(), mae.item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde90d8b-ed0a-4f6e-a504-5b279baa3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:6\", help=\"\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"ETTh1\", help=\"data path\")\n",
    "    parser.add_argument(\"--channel\", type=int, default=512, help=\"number of features\")\n",
    "    parser.add_argument(\"--num_nodes\", type=int, default=7, help=\"number of nodes\")\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=96, help=\"seq_len\")\n",
    "    parser.add_argument(\"--pred_len\", type=int, default=96, help=\"out_len\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"batch size\")\n",
    "    parser.add_argument(\"--lrate\", type=float, default=1e-4, help=\"learning rate\")\n",
    "    parser.add_argument(\"--dropout_n\", type=float, default=0.2, help=\"dropout rate of neural network layers\")\n",
    "    parser.add_argument(\"--d_llm\", type=int, default=768, help=\"hidden dimensions\")\n",
    "    parser.add_argument(\"--e_layer\", type=int, default=1, help=\"layers of transformer encoder\")\n",
    "    parser.add_argument(\"--head\", type=int, default=8, help=\"heads of attention\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"gpt2\", help=\"llm\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-3, help=\"weight decay rate\")\n",
    "    parser.add_argument(\"--feature_w\", type=float, default=0.01, help=\"weight of feature kd loss\")\n",
    "    parser.add_argument(\"--fcst_w\", type=float, default=1, help=\"weight of forecast loss\")\n",
    "    parser.add_argument(\"--recon_w\", type=float, default=0.5, help=\"weight of reconstruction loss\")\n",
    "    parser.add_argument(\"--att_w\", type=float, default=0.01, help=\"weight of attention kd loss\")\n",
    "    parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100, help=\"\")\n",
    "    parser.add_argument('--seed', type=int, default=2036, help='random seed')\n",
    "    parser.add_argument(\n",
    "        \"--es_patience\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"quit if no improvement after this many iterations\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save\",\n",
    "        type=str,\n",
    "        default=\"./logs/\" + str(time.strftime(\"%Y-%m-%d-%H:%M:%S\")) + \"-\",\n",
    "        help=\"save path\",\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf89f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    data_map = {\n",
    "        'ETTh1': Dataset_ETT_hour,\n",
    "        'ETTh2': Dataset_ETT_hour,\n",
    "        'ETTm1': Dataset_ETT_minute,\n",
    "        'ETTm2': Dataset_ETT_minute\n",
    "        }\n",
    "    data_class = data_map.get(args.data_path, Dataset_Custom)\n",
    "    train_set = data_class(flag='train', scale=True, size=[args.seq_len, 0, args.pred_len], data_path=args.data_path)\n",
    "    val_set = data_class(flag='val', scale=True, size=[args.seq_len, 0, args.pred_len], data_path=args.data_path)\n",
    "    test_set = data_class(flag='test', scale=True, size=[args.seq_len, 0, args.pred_len], data_path=args.data_path)\n",
    "    \n",
    "    scaler = train_set.scaler\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=False, drop_last=True, num_workers=args.num_workers)\n",
    "    val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, drop_last=True, num_workers=args.num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, drop_last=False, num_workers=args.num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1260332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9157a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_provider.data_loader_emb import Dataset_ETT_hour #, Dataset_ETT_minute, Dataset_Custom\n",
    "# from model.TimeKD import Dual\n",
    "# from utils.kd_loss import KDLoss\n",
    "# from utils.metrics import MSE, MAE, metric\n",
    "# import faulthandler\n",
    "# faulthandler.enable()\n",
    "# torch.cuda.empty_cache()\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:150\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e058a880-7811-461c-8ba2-20c6e18f455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43091e-0c70-44d7-8b8f-2d77ac061f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dcbed01",
   "metadata": {},
   "source": [
    "### test h5 for 3D matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fb3bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d45c3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([34441, 7, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file_path = '/home/zongqi/004_TimeKD/TimeKD/ETTm1/24/train_stack/0.h5'\n",
    "\n",
    "# with h5py.File(file_path, 'r') as hf:\n",
    "#     data = hf['stacked_embeddings'][:]\n",
    "#     tensor = torch.from_numpy(data)\n",
    "\n",
    "# tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7273842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/home/zongqi/004_TimeKD/TimeKD/ETTm1/24/train/0.h5'\n",
    "\n",
    "with h5py.File(file_path, 'r') as hf:\n",
    "    data = hf['embeddings'][:]\n",
    "    tensor = torch.from_numpy(data)\n",
    "\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4839a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_stack = []\n",
    "embeddings = tensor\n",
    "embeddings_stack.append(tensor.squeeze(0).detach())\n",
    "embeddings_stack.append(tensor.squeeze(0).detach())\n",
    "\n",
    "stacked_embeddings = torch.stack(embeddings_stack, dim=0)\n",
    "stacked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = '/home/zongqi/004_TimeKD/TimeKD/test_h5.h5'\n",
    "# with h5py.File(file_path, 'w') as hf:\n",
    "#     hf.create_dataset('embeddings', data=stacked_embeddings.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d51946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with h5py.File(file_path, 'r') as hf:\n",
    "#     data = hf['embeddings'][:]\n",
    "#     tensor = torch.from_numpy(data)\n",
    "\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f981b85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = stacked_embeddings\n",
    "\n",
    "temp_list = []\n",
    "temp_list.append(temp)\n",
    "temp_list.append(temp)\n",
    "\n",
    "stacked_temp = torch.cat(temp_list, dim=0)\n",
    "stacked_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131596c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f69cbe50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 7, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/home/zongqi/004_TimeKD/TimeKD/ETTm1/24/train_batch/batch.h5'\n",
    "\n",
    "with h5py.File(file_path, 'r') as hf:\n",
    "    data = hf['stacked_embeddings'][:]\n",
    "    tensor = torch.from_numpy(data)\n",
    "\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444bc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TimeKD_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
