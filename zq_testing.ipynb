{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9fd263f-0f92-490b-b8ce-5a67f3f6e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        scaler,\n",
    "        channel,\n",
    "        num_nodes,\n",
    "        seq_len,\n",
    "        pred_len,\n",
    "        dropout_n,\n",
    "        d_llm,\n",
    "        e_layer,\n",
    "        head,\n",
    "        lrate,\n",
    "        wdecay,\n",
    "        feature_w,\n",
    "        fcst_w,\n",
    "        recon_w,\n",
    "        att_w,\n",
    "        device,\n",
    "        epochs\n",
    "    ):\n",
    "        self.model = Dual(\n",
    "            device=device, channel=channel, num_nodes=num_nodes, seq_len=seq_len, pred_len=pred_len, \n",
    "            dropout_n=dropout_n, d_llm=d_llm, e_layer=e_layer, head=head\n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.AdamW(self.model.parameters(), lr=lrate, weight_decay=wdecay)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=min(epochs, 100), eta_min=1e-8, verbose=True)\n",
    "        self.MSE = MSE\n",
    "        self.MAE = MAE\n",
    "        self.clip = 5\n",
    "        self.scaler = scaler\n",
    "        self.device = device\n",
    "\n",
    "        self.feature_loss = 'smooth_l1'  \n",
    "        self.fcst_loss = 'smooth_l1'\n",
    "        self.recon_loss = 'smooth_l1'\n",
    "        self.att_loss = 'smooth_l1'   \n",
    "        self.fcst_w = 1\n",
    "        self.recon_w = 0.5\n",
    "        self.feature_w = 0.1     \n",
    "        self.att_w = 0.01\n",
    "        self.criterion = KDLoss(self.feature_loss, self.fcst_loss, self.recon_loss, self.att_loss,  self.feature_w,  self.fcst_w,  self.recon_w,  self.att_w)\n",
    "\n",
    "        # print(\"The number of trainable parameters: {}\".format(self.model.count_trainable_params()))\n",
    "        print(\"The number of parameters: {}\".format(self.model.param_num()))\n",
    "        print(self.model)\n",
    "\n",
    "    def train(self, x, y, emb):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        ts_enc, prompt_enc, ts_out, prompt_out, ts_att, prompt_att = self.model(x, emb)\n",
    "        loss = self.criterion(ts_enc, prompt_enc, ts_out, prompt_out, ts_att, prompt_att, y)\n",
    "        loss.backward()\n",
    "        if self.clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.clip) \n",
    "        self.optimizer.step() \n",
    "        mse = self.MSE(ts_out, y) \n",
    "        mae = self.MAE(ts_out, y)\n",
    "        return loss.item(), mse.item(), mae.item()\n",
    "\n",
    "    def eval(self, x, y, emb):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            ts_enc, prompt_enc, ts_out, prompt_out, ts_att, prompt_att = self.model(x, emb)\n",
    "            loss = self.criterion(ts_enc, prompt_enc, ts_out, prompt_out, ts_att, prompt_att, y)\n",
    "            mse = self.MSE(ts_out, y)\n",
    "            mae = self.MAE(ts_out, y)\n",
    "        return loss.item(), mse.item(), mae.item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bde90d8b-ed0a-4f6e-a504-5b279baa3d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda:6\", help=\"\")\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"ETTh1\", help=\"data path\")\n",
    "    parser.add_argument(\"--channel\", type=int, default=512, help=\"number of features\")\n",
    "    parser.add_argument(\"--num_nodes\", type=int, default=7, help=\"number of nodes\")\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=96, help=\"seq_len\")\n",
    "    parser.add_argument(\"--pred_len\", type=int, default=96, help=\"out_len\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32, help=\"batch size\")\n",
    "    parser.add_argument(\"--lrate\", type=float, default=1e-4, help=\"learning rate\")\n",
    "    parser.add_argument(\"--dropout_n\", type=float, default=0.2, help=\"dropout rate of neural network layers\")\n",
    "    parser.add_argument(\"--d_llm\", type=int, default=768, help=\"hidden dimensions\")\n",
    "    parser.add_argument(\"--e_layer\", type=int, default=1, help=\"layers of transformer encoder\")\n",
    "    parser.add_argument(\"--head\", type=int, default=8, help=\"heads of attention\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"gpt2\", help=\"llm\")\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=1e-3, help=\"weight decay rate\")\n",
    "    parser.add_argument(\"--feature_w\", type=float, default=0.01, help=\"weight of feature kd loss\")\n",
    "    parser.add_argument(\"--fcst_w\", type=float, default=1, help=\"weight of forecast loss\")\n",
    "    parser.add_argument(\"--recon_w\", type=float, default=0.5, help=\"weight of reconstruction loss\")\n",
    "    parser.add_argument(\"--att_w\", type=float, default=0.01, help=\"weight of attention kd loss\")\n",
    "    parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')\n",
    "    parser.add_argument(\"--epochs\", type=int, default=100, help=\"\")\n",
    "    parser.add_argument('--seed', type=int, default=2036, help='random seed')\n",
    "    parser.add_argument(\n",
    "        \"--es_patience\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"quit if no improvement after this many iterations\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--save\",\n",
    "        type=str,\n",
    "        default=\"./logs/\" + str(time.strftime(\"%Y-%m-%d-%H:%M:%S\")) + \"-\",\n",
    "        help=\"save path\",\n",
    "    )\n",
    "    return parser.parse_args()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf89f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    data_map = {\n",
    "        'ETTh1': Dataset_ETT_hour,\n",
    "        'ETTh2': Dataset_ETT_hour,\n",
    "        'ETTm1': Dataset_ETT_minute,\n",
    "        'ETTm2': Dataset_ETT_minute\n",
    "        }\n",
    "    data_class = data_map.get(args.data_path, Dataset_Custom)\n",
    "    train_set = data_class(flag='train', scale=True, size=[args.seq_len, 0, args.pred_len], data_path=args.data_path)\n",
    "    val_set = data_class(flag='val', scale=True, size=[args.seq_len, 0, args.pred_len], data_path=args.data_path)\n",
    "    test_set = data_class(flag='test', scale=True, size=[args.seq_len, 0, args.pred_len], data_path=args.data_path)\n",
    "    \n",
    "    scaler = train_set.scaler\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=False, drop_last=True, num_workers=args.num_workers)\n",
    "    val_loader = DataLoader(val_set, batch_size=args.batch_size, shuffle=False, drop_last=True, num_workers=args.num_workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=1, shuffle=False, drop_last=False, num_workers=args.num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1260332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9157a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data_provider.data_loader_emb import Dataset_ETT_hour #, Dataset_ETT_minute, Dataset_Custom\n",
    "# from model.TimeKD import Dual\n",
    "# from utils.kd_loss import KDLoss\n",
    "# from utils.metrics import MSE, MAE, metric\n",
    "# import faulthandler\n",
    "# faulthandler.enable()\n",
    "# torch.cuda.empty_cache()\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:150\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e058a880-7811-461c-8ba2-20c6e18f455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "# args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43091e-0c70-44d7-8b8f-2d77ac061f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dcbed01",
   "metadata": {},
   "source": [
    "### test h5 for 3D matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8fb3bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "939aaa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Amex_Dataset:\n",
    "    # def __init__(self,df_series,df_feature,uidxs,df_y=None):\n",
    "    def __init__(self,df_series,uidxs,df_y=None,label_name = 'target',id_name = 'customer_ID'):\n",
    "        self.df_series = df_series\n",
    "        # self.df_feature = df_feature\n",
    "        self.df_y = df_y\n",
    "        self.uidxs = uidxs\n",
    "        self.label_name = label_name\n",
    "        self.id_name = id_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.uidxs))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        i1,i2,idx = self.uidxs[index]\n",
    "        series = self.df_series.iloc[i1:i2+1,1:].drop(['S_2'],axis=1).values\n",
    "        time_ref = self.df_series.iloc[i1:i2+1,1:]['S_2']\n",
    "        # series = self.df_series.iloc[i1:i2+1,1:].drop(['year_month','S_2'],axis=1).values\n",
    "\n",
    "        if len(series.shape) == 1:\n",
    "            series = series.reshape((-1,)+series.shape[-1:])\n",
    "        # series_ = series.copy()\n",
    "        # series_[series_!=0] = 1.0 - series_[series_!=0] + 0.001\n",
    "        # feature = self.df_feature.loc[idx].values[1:]\n",
    "        # feature_ = feature.copy()\n",
    "        # feature_[feature_!=0] = 1.0 - feature_[feature_!=0] + 0.001\n",
    "        \n",
    "        emb_path = f\"/export/home2/zongqi001/004_TimeKD/TimeKD/amex_emb/train/\"\n",
    "        file_path = os.path.join(emb_path, f\"{idx}.h5\")\n",
    "\n",
    "        with h5py.File(file_path, 'r') as hf:\n",
    "            emb_data = hf['stacked_embeddings'][:]\n",
    "            emb_tensor = torch.from_numpy(emb_data)\n",
    "\n",
    "        if self.df_y is not None:\n",
    "            label = self.df_y.loc[idx,[self.label_name]].values\n",
    "            return {\n",
    "                    'SERIES': series,#np.concatenate([series,series_],axis=1),\n",
    "                    # 'FEATURE': np.concatenate([feature,feature_]),\n",
    "                    'LABEL': label,\n",
    "                    'time_ref': time_ref,\n",
    "                    'idx': idx,\n",
    "                    'emb_tensor': emb_tensor,\n",
    "                    }\n",
    "        else:\n",
    "            return {\n",
    "                    'SERIES': series,#np.concatenate([series,series_],axis=1),\n",
    "                    # 'FEATURE': np.concatenate([feature,feature_]),\n",
    "                    'time_ref': time_ref,\n",
    "                    'idx': idx,\n",
    "                    }\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Padding to same size.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = len(batch)\n",
    "        batch_series = torch.zeros((batch_size, 13, batch[0]['SERIES'].shape[1]))\n",
    "        batch_mask = torch.zeros((batch_size, 13))\n",
    "        # batch_feature = torch.zeros((batch_size, batch[0]['FEATURE'].shape[0]))\n",
    "        batch_y = torch.zeros(batch_size)\n",
    "        batch_time_ref = np.array([sample['time_ref'] for sample in batch])\n",
    "        batch_idx = np.array([sample['idx'] for sample in batch])\n",
    "        batch_emb_tensor = None\n",
    "\n",
    "        for i, item in enumerate(batch):\n",
    "            v = item['SERIES']\n",
    "            batch_series[i, :v.shape[0], :] = torch.tensor(v).float()\n",
    "            batch_mask[i,:v.shape[0]] = 1.0\n",
    "            # v = item['FEATURE'].astype(np.float32)\n",
    "            # batch_feature[i] = torch.tensor(v).float()\n",
    "            if self.df_y is not None:\n",
    "                v = item['LABEL'].astype(np.float32)\n",
    "                batch_y[i] = torch.tensor(v).float()\n",
    "                batch_emb_tensor = torch.stack([sample['emb_tensor'] for sample in batch], dim=0) \n",
    "\n",
    "        return {'batch_series':batch_series\n",
    "                ,'batch_mask':batch_mask\n",
    "                # ,'batch_feature':batch_feature\n",
    "                ,'batch_y':batch_y\n",
    "                ,'batch_time_ref':batch_time_ref\n",
    "                ,'batch_idx':batch_idx\n",
    "                ,'batch_emb_tensor':batch_emb_tensor\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7a8385",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/export/home2/zongqi001/000_data/amex/13month_0.1pct'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee341c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_series     = pd.read_feather(f'{input_path}/df_nn_series_train.feather')\n",
    "train_series_idx = pd.read_feather(f'{input_path}/df_nn_series_idx_train.feather').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a66fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.read_csv(f'{input_path}/train_labels.csv')\n",
    "train_dataset = Amex_Dataset(train_series,train_series_idx,train_y)\n",
    "train_dataloader = DataLoader(train_dataset,batch_size=4,shuffle=True, drop_last=False, collate_fn=train_dataset.collate_fn,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94d93fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8be21543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000f8675ede66cc6affd4c048db11a00246d7ee623f453...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00def60d36bbb3f6a51dcf0e8a999ab2c383813ec7e8ca...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         customer_ID  target\n",
       "0  000f8675ede66cc6affd4c048db11a00246d7ee623f453...       0\n",
       "1  00def60d36bbb3f6a51dcf0e8a999ab2c383813ec7e8ca...       1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a084aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26, 38,  2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_series_idx[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333043e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_1pct\n"
     ]
    }
   ],
   "source": [
    "sampling = None\n",
    "sampling = '1pct'\n",
    "\n",
    "s = f'data_{sampling}' if sampling else f'data'\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93d3d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold,GroupKFold\n",
    "skf = StratifiedKFold(n_splits = 2, shuffle=True, random_state=42)\n",
    "fold1, fold2 = skf.split(train_y,train_y['target'])\n",
    "trn_index, val_index = fold1[0], fold1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "059cc60d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   3,   5,   6,   7,   9,  12,  13,  15,  17,  19,  22,  23,\n",
       "        29,  34,  39,  40,  41,  42,  45,  46,  48,  49,  50,  51,  52,\n",
       "        53,  55,  58,  60,  61,  62,  65,  66,  68,  69,  70,  72,  74,\n",
       "        77,  81,  86,  90,  92,  96,  97,  99, 100, 102, 105, 107, 110,\n",
       "       114, 117, 120, 121, 122, 123, 124, 126, 130, 131, 133, 136, 137,\n",
       "       138, 139, 142, 144, 152, 156, 157, 158, 160, 161, 162, 163, 165,\n",
       "       166, 169, 170, 172, 174, 175, 176, 178, 179, 183, 185, 186, 189,\n",
       "       192, 193, 194, 196, 199, 200, 201, 202, 203, 204, 205, 207, 208,\n",
       "       209, 210, 214, 216, 217])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c220956b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   4,   8,  10,  11,  14,  16,  18,  20,  21,  24,  25,\n",
       "        26,  27,  28,  30,  31,  32,  33,  35,  36,  37,  38,  43,  44,\n",
       "        47,  54,  56,  57,  59,  63,  64,  67,  71,  73,  75,  76,  78,\n",
       "        79,  80,  82,  83,  84,  85,  87,  88,  89,  91,  93,  94,  95,\n",
       "        98, 101, 103, 104, 106, 108, 109, 111, 112, 113, 115, 116, 118,\n",
       "       119, 125, 127, 128, 129, 132, 134, 135, 140, 141, 143, 145, 146,\n",
       "       147, 148, 149, 150, 151, 153, 154, 155, 159, 164, 167, 168, 171,\n",
       "       173, 177, 180, 181, 182, 184, 187, 188, 190, 191, 195, 197, 198,\n",
       "       206, 211, 212, 213, 215])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20faa921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# for iter, data in enumerate(tqdm(train_dataloader)):\n",
    "#     print(iter, len(data['batch_series']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd2594d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_series     = pd.read_feather(f'{input_path}/df_nn_series_test.feather')\n",
    "test_series_idx = pd.read_feather(f'{input_path}/df_nn_series_idx_test.feather').values\n",
    "test_y = pd.read_csv(f'{input_path}/test_labels.csv')['target']\n",
    "\n",
    "test_dataset = Amex_Dataset(test_series,test_series_idx)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=4,shuffle=True, drop_last=False, collate_fn=test_dataset.collate_fn,num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2103c219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ca1af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13, 25,  1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_series_idx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cfd6e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 21.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4\n",
      "1 4\n",
      "2 4\n",
      "3 4\n",
      "4 4\n",
      "5 4\n",
      "6 4\n",
      "7 4\n",
      "8 4\n",
      "9 4\n",
      "10 4\n",
      "11 4\n",
      "12 4\n",
      "13 4\n",
      "14 4\n",
      "15 4\n",
      "16 4\n",
      "17 4\n",
      "18 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for iter1, data in enumerate(tqdm(test_dataloader)):\n",
    "    print(iter1, len(data['batch_series']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c940bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0edfcc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home2/zongqi001/001_env/001_miniconda/001_mininconda/envs/amex__TimeKD/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "from utils.tools import StandardScaler\n",
    "from model.TimeKD import Dual\n",
    "from utils.kd_loss import KDLoss\n",
    "from utils.metrics import MSE, MAE, metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "252d4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/home2/zongqi001/001_env/001_miniconda/001_mininconda/envs/amex__TimeKD/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-04.\n",
      "The number of parameters: 8809260\n",
      "Dual(\n",
      "  (normalize_layers): Normalize()\n",
      "  (length_to_feature): Linear(in_features=13, out_features=512, bias=True)\n",
      "  (token_to_feature): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (ts_encoder): Encoder(\n",
      "    (attn_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (attention): AttentionLayer(\n",
      "          (inner_attention): FullAttention(\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (conv1): Conv1d(512, 3072, kernel_size=(1,), stride=(1,))\n",
      "        (conv2): Conv1d(3072, 512, kernel_size=(1,), stride=(1,))\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (prompt_encoder): Encoder(\n",
      "    (attn_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (attention): AttentionLayer(\n",
      "          (inner_attention): FullAttention(\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "          )\n",
      "          (query_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (key_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (value_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_projection): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (conv1): Conv1d(512, 3072, kernel_size=(1,), stride=(1,))\n",
      "        (conv2): Conv1d(3072, 512, kernel_size=(1,), stride=(1,))\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (ts_proj): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (ts_proj2): Sequential(\n",
      "    (0): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=220, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (prompt_proj): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (prompt_proj2): Sequential(\n",
      "    (0): LayerNorm((220,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Linear(in_features=220, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "engine = trainer(\n",
    "    scaler=StandardScaler,\n",
    "    channel=512,\n",
    "    num_nodes=220,\n",
    "    seq_len=13,\n",
    "    pred_len=1,\n",
    "    dropout_n=0.2,\n",
    "    d_llm=768,\n",
    "    e_layer=1,\n",
    "    head=8,\n",
    "    lrate=1e-4,\n",
    "    wdecay=1e-3,\n",
    "    feature_w=0.01,\n",
    "    fcst_w=1,\n",
    "    recon_w=0.5,\n",
    "    att_w=0.01,\n",
    "    device=\"cpu\",\n",
    "    epochs=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c9e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [00:01<00:00, 47.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# for iter1, data_last in enumerate(tqdm(train_dataloader)):\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d04213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 22.59it/s]\n"
     ]
    }
   ],
   "source": [
    "for iter1, data_last in enumerate(tqdm(test_dataloader)):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f1e38ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13, 220])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['batch_series'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f6c8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['batch_y']\n",
    "x = data['batch_series']\n",
    "emb_tensor = data['batch_emb_tensor']\n",
    "device = 'cpu'\n",
    "trainx = torch.Tensor(x).to(device).float()\n",
    "trainy = torch.Tensor(y).to(device).float()\n",
    "emb = torch.Tensor(emb_tensor).to(device).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aee91ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = engine.train(trainx, trainy, emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77d765d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12315793335437775, 0.2463158667087555, 0.47641435265541077)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f55e2756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 13, 220])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aaa = next(iter(test_dataloader))\n",
    "aaa['batch_series'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05679e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_series\n",
      "batch_mask\n",
      "batch_y\n",
      "batch_time_ref\n",
      "batch_idx\n",
      "batch_emb_tensor\n"
     ]
    }
   ],
   "source": [
    "for item in aaa:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c23b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9fe9e0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/export/home2/zongqi001/004_TimeKD/TimeKD/logs/Amex/1_512_1_0.0001_0.2_42_0.01/best_model.pth'\n",
    "engine.model.load_state_dict(torch.load(path, map_location=torch.device('cpu')), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95e5922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testx shape: torch.Size([4, 13, 220])\n",
      "preds len: torch.Size([4])\n",
      "preds: (tensor([[[ 0.6451, -0.0119, -0.9287,  ..., -0.2492,  0.7468, -0.9597],\n",
      "         [ 0.6600,  0.2520, -0.7379,  ...,  0.0831,  0.3457, -0.6377],\n",
      "         [ 1.6661, -0.3234, -0.7198,  ...,  1.0208,  0.3167,  0.0920],\n",
      "         ...,\n",
      "         [ 1.6447,  0.2347, -0.8127,  ...,  0.1912,  0.0680, -0.5167],\n",
      "         [ 1.4680,  0.5246, -0.4868,  ...,  0.2213,  0.1872,  0.2498],\n",
      "         [ 1.4589, -0.3334,  0.4941,  ...,  0.5517,  0.3376, -0.2136]],\n",
      "\n",
      "        [[ 0.4480,  0.4861, -0.2290,  ..., -1.9047,  0.5211,  0.2954],\n",
      "         [ 0.6513, -0.6885,  0.6580,  ...,  0.0853, -0.3880,  0.2479],\n",
      "         [ 2.8270,  0.2287, -1.4148,  ...,  0.9289, -0.4948, -0.1103],\n",
      "         ...,\n",
      "         [ 0.8056,  0.3727, -0.4812,  ...,  0.4020,  0.3636,  0.3260],\n",
      "         [ 1.7412,  0.3133,  0.4320,  ...,  0.5747,  0.4761, -0.4457],\n",
      "         [ 1.8720, -0.1302, -0.9207,  ...,  0.2930,  0.5951, -0.2599]],\n",
      "\n",
      "        [[ 0.5499, -0.1574, -0.6122,  ..., -0.5587, -0.6681, -0.5462],\n",
      "         [ 1.7949,  0.3465, -0.9267,  ...,  0.6950,  0.1953,  0.1621],\n",
      "         [ 1.0695,  0.3392, -0.9689,  ...,  0.4890,  0.0986,  0.3104],\n",
      "         ...,\n",
      "         [ 2.0784,  0.2297,  0.3313,  ...,  0.6031,  0.1512,  0.2238],\n",
      "         [ 2.1998, -0.1491, -0.9754,  ..., -0.2801,  0.2932, -0.3315],\n",
      "         [ 1.2174, -0.5758, -0.6649,  ..., -0.6614, -0.2495,  0.0151]],\n",
      "\n",
      "        [[ 1.2581, -0.2194, -1.7039,  ..., -0.6879,  0.8650, -1.1739],\n",
      "         [ 1.9545,  0.5653, -0.5967,  ..., -1.1675, -0.1124, -0.6967],\n",
      "         [ 2.0902, -0.6427, -0.7203,  ..., -0.7143, -0.3084, -0.6225],\n",
      "         ...,\n",
      "         [ 0.0943, -1.1799, -0.6471,  ..., -0.5350, -0.1717,  0.0080],\n",
      "         [ 1.4614, -0.5203,  0.4375,  ...,  0.1874,  0.5716, -0.6930],\n",
      "         [ 1.9067,  0.0127, -0.8525,  ...,  0.2155,  0.5493, -0.3107]]]), None, tensor([0.0040, 0.0101, 0.0189, 0.0311]), None, None, None)\n"
     ]
    }
   ],
   "source": [
    "data = aaa\n",
    "\n",
    "x = data['batch_series']\n",
    "\n",
    "testx = torch.Tensor(x).to(device).float()\n",
    "\n",
    "print('testx shape:', testx.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = engine.model(testx, None)\n",
    "print('preds len:', preds[2].shape)\n",
    "print('preds:', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2968c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testx.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42444230",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96582dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testx shape: torch.Size([1, 13, 220])\n",
      "preds len: torch.Size([])\n",
      "preds: (tensor([[[ 0.0096, -1.4032,  1.0565,  ..., -0.2951, -0.7156, -0.4563],\n",
      "         [-0.1042,  0.0216,  0.9074,  ..., -1.5325,  0.0642, -1.1931],\n",
      "         [ 0.0074, -1.0180, -0.1519,  ..., -1.1090,  0.3688, -0.7764],\n",
      "         ...,\n",
      "         [ 0.1152, -0.1989,  0.3614,  ..., -0.8293,  0.0494, -0.7065],\n",
      "         [ 0.7021, -0.8486, -0.1324,  ..., -0.6438, -0.6686, -0.5303],\n",
      "         [-0.0576, -0.7737, -0.8084,  ..., -2.5327,  1.8250, -0.4494]]]), None, tensor(0.9931), None, None, None)\n"
     ]
    }
   ],
   "source": [
    "# data = aaa\n",
    "data = data_last\n",
    "\n",
    "x = data['batch_series']\n",
    "\n",
    "testx = torch.Tensor(x).to(device).float()\n",
    "\n",
    "print('testx shape:', testx.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = engine.model(testx, None)\n",
    "print('preds len:', preds[2].shape)\n",
    "print('preds:', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "733738b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9931])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if testx.shape[0] == 1:\n",
    "    pred_y =  torch.tensor([preds[2]])\n",
    "else:\n",
    "    pred_y = preds[2]\n",
    "\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51d3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c712a0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d45c3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([34441, 7, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/export/home2/zongqi001/004_TimeKD/TimeKD/ETTm1/24/train_batch/batch.h5'\n",
    "\n",
    "with h5py.File(file_path, 'r') as hf:\n",
    "    data = hf['stacked_embeddings'][:]\n",
    "    tensor = torch.from_numpy(data)\n",
    "\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7273842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([220, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/export/home2/zongqi001/004_TimeKD/TimeKD/amex_emb/train/0.h5'\n",
    "\n",
    "with h5py.File(file_path, 'r') as hf:\n",
    "    data = hf['stacked_embeddings'][:]\n",
    "    tensor = torch.from_numpy(data)\n",
    "\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e00cdd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 220, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_stack = [tensor.squeeze(0).detach(), tensor.squeeze(0).detach()]\n",
    "stacked_embeddings = torch.stack(embeddings_stack, dim=0)\n",
    "stacked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d4839a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 768])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_stack = []\n",
    "embeddings = tensor\n",
    "embeddings_stack.append(tensor.squeeze(0).detach())\n",
    "embeddings_stack.append(tensor.squeeze(0).detach())\n",
    "\n",
    "stacked_embeddings = torch.stack(embeddings_stack, dim=0)\n",
    "stacked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4962c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = '/home/zongqi/004_TimeKD/TimeKD/test_h5.h5'\n",
    "# with h5py.File(file_path, 'w') as hf:\n",
    "#     hf.create_dataset('embeddings', data=stacked_embeddings.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d51946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 7, 768)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with h5py.File(file_path, 'r') as hf:\n",
    "#     data = hf['embeddings'][:]\n",
    "#     tensor = torch.from_numpy(data)\n",
    "\n",
    "# data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f981b85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 7, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = stacked_embeddings\n",
    "\n",
    "temp_list = []\n",
    "temp_list.append(temp)\n",
    "temp_list.append(temp)\n",
    "\n",
    "stacked_temp = torch.cat(temp_list, dim=0)\n",
    "stacked_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131596c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f69cbe50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 7, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = '/home/zongqi/004_TimeKD/TimeKD/ETTm1/24/train_batch/batch.h5'\n",
    "\n",
    "with h5py.File(file_path, 'r') as hf:\n",
    "    data = hf['stacked_embeddings'][:]\n",
    "    tensor = torch.from_numpy(data)\n",
    "\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4444bc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amex__TimeKD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
